---
title: "Homework 7"
output:
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(dplyr)
```

<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>


## Please submit the solution in the form of R Markdown report, knitted into either of the available formats (HTML, pdf or Word). Provide all relevant code and output. Goal of this homework is to have you familiarized with categorical predictors (dummy variables); incremental F-test.



# Problem #1

For the Wage data set from ISLR package, let’s work on several models:

1) Explaining wage (in 1, 000$s) as a function of age and job class.
    a. Proceed to write down the full modeling equation for wage $\sim$ age + jobclass regression, properly explaining the dummy variables.
    b. Fit the model from (a), and write down the fitted equation.
    c. Is job class a statistically significant variable? Why? If yes - proceed to interpret its effect on wage.
    
```{r}
lm.obj <- lm(wage ~ age + jobclass, data = ISLR::Wage)
summary(lm.obj)$coefficients %>% knitr::kable(booktabs = T)
```
  a. $wage_i = \beta_0+\beta_1age_i+\beta_2D_{job,i}+\epsilon_i=\left\{ \begin{array}{cl}\beta_0+\beta_1age_i+\beta_2+\epsilon_i \\ \beta_0+\beta_1age_i+\epsilon_i \end{array} \right.,$\
  $\epsilon_i\sim N(0,\sigma^2)$\
  $D_{job,i}=\left\{ \begin{array}{cl}1,&i^{th}~worker~has~Information~job \\ 0,&i^{th}~worker~has~Industrial~job \end{array} \right.$\
  b. $\hat{wage} =$ `r lm.obj$coefficients[1]` $+$ `r lm.obj$coefficients[2]` $age+$ `r lm.obj$coefficients[3]` $D_{job,i}$\
  c.
  $H_0:\beta_2=0$\
  $H_a:\beta_2\ne0$\
  Job class is a significant variable because the p-value (`r summary(lm.obj)$coefficients[3,4]`$\approx0$) is less than all common significance levels ($\alpha=0.1,0.05,0.001$). For male workers in the Mid-Atlantic region with Information job types, the average wage will be \$`r as.integer(summary(lm.obj)$coefficients[3,1]*1000)` higher than for those with Industrial job types, holding age constant.
  
2) Explaining wage (in 1, 000$s) as a function of age and marital status
    a. Proceed to write down the full modeling equation for wage $\sim$ age + maritl regression, properly explaining the dummy variables.
    b. Fit the model from (a), and write down the fitted equation
    c. Comment on the statistical significance of each dummy variable
    d. Interpret the most statistically significant dummy variable (NOT the "per 1-unit" version though, the group comparison version).
    e. Conduct the test for significance of the entire marital variable when predicting person’s wage. In particular, make sure to 1) formulate the H0, Ha hypotheses; 2) write down the modeling equation of the “null” model; 3) write the formula for test statistic; 4) use R’s anova() to carry out the test, and match the output to the terms in the test statistic formula; 5) provide the conclusion of the test.
    
```{r}
lm.obj <- lm(wage ~ age + maritl, data = ISLR::Wage)
summary(lm.obj)$coefficients %>% knitr::kable(booktabs = T)
```
  a. \
  $wage_i = \beta_0+\beta_1age_i+\beta_2D_{married,i}+\beta_3D_{widowed,i}+\beta_4D_{divorced,i}+\beta_5D_{separated,i}+\epsilon_i,~\epsilon_i\sim N(0,\sigma^2)$\
  $D_{married,i}=\left\{ \begin{array}{cl}1,&marital~status~of~i^{th}~worker~is~married \\ 0,&otherwise \end{array} \right.$\
  $D_{widowed,i}=\left\{ \begin{array}{cl}1,&marital~status~of~i^{th}~worker~is~widowed \\ 0,&otherwise \end{array} \right.$\
  $D_{divorced,i}=\left\{ \begin{array}{cl}1,&marital~status~of~i^{th}~worker~is~divorced \\ 0,&otherwise \end{array} \right.$\
  $D_{separated,i}=\left\{ \begin{array}{cl}1,&marital~status~of~i^{th}~worker~is~separated \\ 0,&otherwise \end{array} \right.$\
  b. \
  $\hat{wage} =$ `r lm.obj$coefficients[1]` $+$ `r lm.obj$coefficients[2]` $age+$ `r lm.obj$coefficients[3]` $D_{married,i}+$\ `r lm.obj$coefficients[4]` $D_{widowed,i}+$\ `r lm.obj$coefficients[5]` $D_{divorced,i}+$\ `r lm.obj$coefficients[6]` $D_{separated,i}$
  c. \
  $D_{married,i}:$ significant dummy variable because the p-value is less than all common significance levels ($\alpha=0.1,0.05,0.001$)\
  $D_{widowed,i}:$ _NOT_ a significant dummy variable because the p-value is greater than all common significance levels ($\alpha=0.1,0.05,0.001$)\
  $D_{divorced,i}:$ _NOT_ a significant dummy variable because the p-value is greater than all common significance levels ($\alpha=0.1,0.05,0.001$)\
  $D_{separated,i}:$ _NOT_ a significant dummy variable because the p-value is greater than all common significance levels ($\alpha=0.1,0.05,0.001$)
  d. \
  For male workers in the Mid-Atlantic region who are married, the average wage will be \$`r as.integer(summary(lm.obj)$coefficients[3,1]*1000)` higher than for those who never married, holding age 
  e. \
  $H_0:\beta_2=...=\beta_5=0$\
  $H_a:\{at~least~one~\beta_j\ne0,~j=2..5\}$\
  $null~model:wage_i = \beta_0+\beta_1age_i+\epsilon_i,~\epsilon_i\sim N(0,\sigma^2)$\
  $test~statistic:FS=\frac{(RegSS_{full}=RegSS_{null})/q}{RSS_{full}/(n-p-1)}$
```{r}
anova(lm(wage ~ age, data = ISLR::Wage), lm.obj)
```
Marital status is a significant variable because the p-value for the incremental f-test ($\approx0$) is less than all common significance levels ($\alpha=0.1,0.05,0.001$). 


# Problem #2

1) When dealing with a categorical predictor X containing > 2 categories (say, 3), why do we have to use dummy variables approach? Why not just model it as
\begin{center}
$\left\{ \begin{array}{cl}X=0,&if~X=\{Categ\#1\} \\ X=1,&if~X=\{Categ\#2\} \\ X=2,&if~X=\{Categ\#3\} \\\end{array} \right.$
\end{center}
By having only one variable with various values, it's effectively treated as a discrete numerical variable during the regression. This effectively would force a linear relationship where higher valued categories result in a higher response value because they share the same $\beta_j$ scaler value. Furthermore, the differences between category effects would be incremental. This effect is not necessarily true and is thus the limitation of using just one label-encoded variable. Dummy variables allow us to more easily isolate the effect of a particular category within X on the response variable. The effect isolation is represented by the potentially different $\beta_j$ values.

2) (+1.5 BONUS PTS) Why, in order to model a categorical predictor with K categories, it is enough to use just K-1 dummy variables, and not K? E.g. why not create a dummy variable for the baseline category as well? For demonstration, proceed to
    a. Create a dummy variable for "Never Married" category
    b. Run the wage maritl regression, but now also using the "Never Married" dummy variable from part (a), in addition to the 4 dummy variables already in place. Print out the fitted model - what do you observe? Why do you think that is?
```{r}
ISLR::Wage %>% 
  mutate(never.married = forcats::fct_other(maritl, keep = "1. Never Married")) %>%
  lm(wage ~ age + maritl + never.married, data = .)
```
The coefficient for the "Never Married" dummy variable is listed as $NA$. This is due to an issue of perfect collinearity because it is impossible to change "Never Married" while holding the other dummy marital status variables constant. R, therefore, automatically ignores / drops the $k^{th}$ dummy variable.