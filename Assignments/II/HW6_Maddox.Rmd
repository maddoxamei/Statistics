---
title: "Homework 6"
output:
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(dplyr)

lm.summary <- function(model, ...){
  
  model.stats <- with(summary(model), c(RSS = sum(residuals^2), RSE = sigma, "$R^2$" = r.squared)) %>% round(3) %>% as.data.frame() %>% rbind(p.value = with(summary(model), pf(fstatistic[1], fstatistic[2], fstatistic[3], lower.tail = F) %>% formatC(format = "e", digits = 3)))
  
  coef.stats <- model %>% 
    broom::tidy() %>%
    tibble::column_to_rownames(var="term") %>%
    mutate(p.value = round(p.value, 5), sign = symnum(p.value, corr = FALSE, na = FALSE, 
                                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), 
                                  symbols = c("***", "**", "*", ".", " ")), p.value = paste(p.value, sign))
  

  knitr::kables(list(
    cbind(confint(model), coef.stats) %>% select(estimate, everything(), -sign) %>%
  kableExtra::kable(valign = 't', booktabs = T, digits = 3) %>%
    kableExtra::add_header_above(c(" ", "Coefficients" = 6), bold = T, italic = T) %>%
    kableExtra::footnote(general = attr(coef.stats$sign, "legend"), general_title = "Signif. codes: ", footnote_as_chunk = T),
  kableExtra::kable(model.stats, col.names = c("value"), booktabs = T, escape = F, valign = 't') %>%
    kableExtra::add_header_above(c("Overall Model" = 2), bold = T, italic = T)
  ), 
  caption = model$call[2] %>% gsub("~", "$\\\\sim$", .)
  )
}

cor.summary <- function(data, threshold = 0.8){
  cor.data <- data %>% 
    select_if(is.numeric) %>% 
    cor() %>% 
    round(digits = 2) %>% 
    as.data.frame() %>%
    mutate_if(is.numeric, function(x) ifelse(x>threshold, x, 0))
  diag(cor.data) <- NA
  cor.data %>%
    filter_all(any_vars(abs(.) > threshold)) %>%
    kableExtra::kable(booktabs = T)
}
```

<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>


## Please submit the solution in the form of R Markdown report, knitted into either of the available formats (HTML, pdf or Word). Provide all relevant code and output. Goal of this homework is to have practice 1) multiple linear regression; 2) quality of fit measures ($R^2$, RSE); 3) collinearity; 4) variable selection; 4) R coding.



# Problem #1

This question involves the use of multiple linear regression on the Auto data set of ISLR library

Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables (except name) as the predictors. Use the summary() function to print the results.

```{r}
lm.obj <- lm(mpg ~ . - name, data = ISLR::Auto)
summary(lm.obj)
```

1) Formulate the H0 and Ha hypotheses (using parameter notation) for testing whether the overall model is significant. Which part of summary() output corresponds to this test? Is the model significant?\
$H_0:\beta_1=...=\beta_7=0$\
$H_a:\{at~least~one~\beta_j\ne0,~j=1,...,7\}$\
The f-statistic section at the bottom corresponds to model significance. The model is statistically significant because the p-value ($\approx0$) is less than most common significance levels ($\alpha=0.001,0.01,0.05$).

2) Which predictors appear to have a statistically significant relationship to the response? Just list them.\
weight, year, and origin appear to have a statistically significant relationship to mpg at the $alpha=0.001$ level. In addition to the previous predictors, displacement appears to additionally have a statistically significant relationship to mpg at the $\alpha=0.01$ level. 

3) Interpret the effect of car’s weight on its miles per gallon.\
There is statistically significant linear relationship between weight (a predictor) and mpg (the response value) when holding all other predictors constant because the p-value ($\approx0$) is less than all common significance levels ($\alpha=0.001,0.01,0.05$). For 1 pound increase in weight, holding all other predictors constant, mpg will decrease by `r abs(summary(lm.obj)$coefficients['weight','Estimate'])` miles per gallon, on average

4) For the effect from part 3, proceed to report and interpret the 95% confidence interval.
```{r}
confint(lm.obj)["weight",]
```
We are 95% confident that for 1 pound increase in weight, holding all other predictors constant, the average mpg will decrease by between `r abs(confint(lm.obj)["weight",][2])` and `r abs(confint(lm.obj)["weight",][1])` miles per gallon.

5) Report and interpret both quality-of-fit metrics.
```{r eval=FALSE}
summary(lm.obj)$r.squared # R-Squared
summary(lm.obj)$sigma # RSE
```
  - $R^2$: Our linear regression model explains `r summary(lm.obj)$r.squared*100` percent of variety in mpg.
  - RSE: Our predicted mpg misses the true population mpg by `r summary(lm.obj)$sigma` miles per gallon, on average

# Problem #2

This problem will deal with cystfibr data example of ISwR package. In particular, we will be building a model to predict pemax (patient’s maximum respiratory pressure) based on other physical characteristics.

1) Proceed to fit the following multiple linear regression model: $pemax\sim~.$
```{r}
lm(pemax ~ ., data = ISwR::cystfibr) %>%
  lm.summary()
```

  a. Comment on the 1) overall model significance; 2) significance of any individual predictors. Why do you think this is happening (name the main issue)?\
The overall linear model is significant at the $\alpha=0.05$ level, indicating at least one important variable, despite no individual predictor being significant. This paradox is due to the issue of collinearity - some predictors are strongly correlated with each other and are thus fighting for effect on the model.
  
  b. Proceed to address the issue observed in part (a) via studying a correlation matrix of predictors, modifying the model accordingly. Fit the modified model, comment on its 1) overall model significance; 2) significance of any individual predictors.
```{r}
cor.summary(ISwR::cystfibr)
cor(ISwR::cystfibr)["pemax", c(1, 3, 4, 7, 8)] %>% data.frame(pemax = .) %>% t() %>%
  kableExtra::kable(booktabs = TRUE)
```
age, weight, and height are are strongly correlated with each other. Similarly, rv and frc are strongly correlated with each other. One one out of each strongly correlated predictor group needs to be retained; I chose the one with the strongest correlation to pemax, the response variable.
```{r}
lm(pemax ~ . - age - height - rv, data = ISwR::cystfibr) %>%
  lm.summary()
```
The overall linear model is significant at the $\alpha=0.01$ level; bmp is significant at the $\alpha=0.05$ level; weight is significant at the $\alpha=0.001$ level. 

  c. Proceed to address the issue observed in part (a) via using VIF criteria method. Fit the modified model, comment on its 1) overall model significance; 2) significance of any individual predictors.
```{r}
car::vif(lm(pemax ~ . , data = ISwR::cystfibr)) # weight highest
car::vif(lm(pemax ~ . - weight, data = ISwR::cystfibr)) # frc highest
car::vif(lm(pemax ~ . - weight - frc, data = ISwR::cystfibr)) # height highest
car::vif(lm(pemax ~ . - weight - frc - height, data = ISwR::cystfibr))
```

```{r}
lm(pemax ~ . - weight - frc - height, data = ISwR::cystfibr) %>%
  lm.summary()
```

The overall linear model is significant at the $\alpha=0.05$ level; fev1 is significant at the $\alpha=0.05$ level; age is significant at the $\alpha=0.01$ level. 
  
  d. In you own words, why does collinearity prevent us from accurately estimating effects of collinear predictors on the response variable?\
  Collinearity prevents us from accurately estimating effects of collinear predictors on the response variable because it is tough to change one predictor while holding a collinear predictor constant since they are correlated. Even if there are observations which meet said criteria, the sample is small and therefore contains more uncertainty.
  
2) Proceed to fit the following two models:\
  - Full model: $pemax\sim~sex + weight + height + rv + frc$
  - Reduced model: $pemax\sim~sex + height + rv$ 
```{r}
models <- list(lm(pemax ~ sex + weight + height + rv + frc, data = ISwR::cystfibr), lm(pemax ~ sex + height + rv, data = ISwR::cystfibr))
names(models) <- c("Full Model", "Reduced Model")
modelsummary::modelsummary(models, output = "latex", estimate = "std.error", statistic = NULL, coef_map = c('height' = 'height', 'rv' = 'rv'), group = model ~ term, title = "Standard Error Comparison")
```
  
  a. Comment on what happens to standard errors for $\hat{\beta_{height}}$ and $\hat{\beta_{frc}}$ coefficients when going from the full to reduced model. Why does this happen?\
  The standard errors shrank when going from the full model to the reduced model. Collinear predictors fight for effect on the response and therefore have more uncertainty which is reflected in increased standard error of respective coefficient estimates (i.e. standard errors get bloated). By removing a collinear predictor, the standard error of the remaining predictor is improved (i.e. shrunk).
  
  b. Proceed to use VIF criteria in order to get from full model down to the reduced model. Which variable is dropped first? Second? Why?
```{r}
car::vif(lm(pemax ~ sex + weight + height + rv + frc, data = ISwR::cystfibr))
```
Weight is dropped first because it has the highest vif value.
```{r}
car::vif(lm(pemax ~ sex + height + rv + frc, data = ISwR::cystfibr))
```
Frc should be dropped second because it has the highest vif value after removing weight.
```{r}
car::vif(lm(pemax ~ sex + height + rv, data = ISwR::cystfibr))
```
No more predictors should be dropped because all the VIF values are below 5.
  
# Problem #3
For Auto data set from ISLR library. Proceed to conduct variable selection via backward AIC approach:

```{r}
lm.obj <- lm(mpg ~ . - name, data = ISLR::Auto)
step(lm.obj)
```
1) Which R function allows us to do that? Which variable(s) ended up being dropped from the model?\
step with the default parameter values allows for the execution of variable selection via backward AIC approach. Acceleration was the only variable which was dropped.

2) Explain what is meant by “Df”, “Sum of Sq”, “RSS” and “AIC” in the tables outputted by step() function.
  - df: by dropping a predictor (a parameter to be estimated), we gain a "degree of freedom"
  - Sum of S: Increase in RSS by going from the full model to the reduced model
  - RSS: RSS for the reduced model

3) Explain why the algorithm stopped (!) on that particular subset of variables. Hint: What does "<none>" represent? Why is it of interest?)\
$<none>$ corresponds to the current full-model since no variables are added or dropped. The resulting table from performing one step is sorted in ascending order according to the AIC value for the model listed on the left. The most optimized model is therefore always the first one listed. The algorithm stopped on that particular subset of variables because $<none>$ (the full-model) was on top (i.e. the full-model had the lowest AIC).

# Problem #4 (BONUS)

This question involves the use of linear regression on the Advertising data set. Proceed to calculate RSS and $R^2$ for the following three models:

  - $Sales\sim~TV$
  - $Sales\sim~TV+radio$
  - $Sales\sim~TV+radio+newspaper$

```{r}
df <- readr::read_csv(sprintf("https://docs.google.com/uc?id=%s&export=download", "1UJIu7Ku3rRWTnFJpK4uLQ2DXW8DjfgtT"))

models <- list(lm(sales ~ TV, data = df),
     lm(sales ~ TV + radio, data = df),
     lm(sales ~ TV + radio + newspaper, data = df))
names(models) <- sapply(models, function(x) as.character(x$call[2]) )

sapply(models, function(x) with(summary(x), c(RSS = sum(residuals^2), "$R^2$" = r.squared))) %>% as.data.frame() %>% t() %>% kableExtra::kable(booktabs = T, esccape = F, digits = 3, caption = "Model Comparison")
```
  
1) Did the RSS decrease (or at least didn’t increase) every time you added an extra variable? Why do you think that is?\
Dropping a variable and its coefficient $\beta_j$ always increases the RSS. RSS metrics represents the _amount of variability in Y left **UNEXPLAINED** by the model_. More predictors means there are more parameters to base the prediction off of, and therefore less uncertainty. For predictors which have no effect on the response value ($\beta_j=0$), removal would not change the RSS as the predictor explains no variability in the first place.

2) Did the $R^2$ increase (or at least didn’t decrease) every time you added an extra variable? Why do you think that is? Hint: use the definition of $R^2$ + part (a).\
$R^2$ represents the _% of variability in Y that’s **EXPLAINED** by the regression model_. It is calculated by $R^2=\frac{TSS-RSS}{TSS}$. TSS is always the same regardless of which predictors are included/excluded from a model for a particular response variable because it ignores the predictors and only looking at the response values and the mean of that response variable; TSS it is static. $R^2=\frac{TSS-RSS}{TSS}=\frac{constant-RSS}{constant}$. Because TSS can be considered a constant, $R^2$ will increase if $RSS$ decreases and visa-versa. As explained earlier, $RSS$ will decrease when variables are added. Therefore, $R^2$ will increase (or at least not decrease) when variables are added.



