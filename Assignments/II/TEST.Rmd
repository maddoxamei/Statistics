---
title: "Homework 6"
output:
  pdf_document: default
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(warning = FALSE)
library(dplyr)

lm.summary <- function(model, ...){
  
  model.stats <- with(summary(model), c(RSS = sum(residuals^2), RSE = sigma, "$R^2$" = r.squared)) %>% round(3) %>% as.data.frame() %>% rbind(p.value = with(summary(model), pf(fstatistic[1], fstatistic[2], fstatistic[3], lower.tail = F) %>% formatC(format = "e", digits = 3)))
  
  coef.stats <- model %>% 
    broom::tidy() %>%
    tibble::column_to_rownames(var="term") %>%
    mutate(p.value = round(p.value, 5), sign = symnum(p.value, corr = FALSE, na = FALSE, 
                                  cutpoints = c(0, 0.001, 0.01, 0.05, 0.1, 1), 
                                  symbols = c("***", "**", "*", ".", " ")), p.value = paste(p.value, sign))
  

  knitr::kables(list(
    cbind(confint(model), coef.stats) %>% select(estimate, everything(), -sign) %>%
  kableExtra::kable(valign = 't', booktabs = T, digits = 3) %>%
    kableExtra::add_header_above(c(" ", "Coefficients" = 6), bold = T, italic = T) %>%
    kableExtra::footnote(general = attr(coef.stats$sign, "legend"), general_title = "Signif. codes: ", footnote_as_chunk = T),
  kableExtra::kable(model.stats, col.names = c("value"), booktabs = T, escape = F, valign = 't') %>%
    kableExtra::add_header_above(c("Overall Model" = 2), bold = T, italic = T)
  ), 
  caption = model$call[2] %>% gsub("~", "$\\\\sim$", .)
  )
}

cor.summary <- function(data, threshold = 0.8){
  cor.data <- data %>% 
    select_if(is.numeric) %>% 
    cor() %>% 
    round(digits = 2) %>% 
    as.data.frame() %>%
    mutate_if(is.numeric, function(x) ifelse(x>threshold, x, 0))
  diag(cor.data) <- NA
  cor.data %>%
    filter_all(any_vars(abs(.) > threshold)) %>%
    kableExtra::kable(booktabs = T)
}
```

<style type="text/css">
    ol { list-style-type: lower-alpha; }
</style>


## Please submit the solution in the form of R Markdown report, knitted into either of the available formats (HTML, pdf or Word). Provide all relevant code and output. Goal of this homework is to have practice 1) multiple linear regression; 2) quality of fit measures ($R^2$, RSE); 3) collinearity; 4) variable selection; 4) R coding.

# Problem #1

This question involves the use of multiple linear regression on the Auto data set of ISLR library

Use the lm() function to perform a multiple linear regression with mpg as the response and all other variables (except name) as the predictors. Use the summary() function to print the results.

```{r}
lm.obj <- lm(mpg ~ . - name, data = ISLR::Auto)
summary(lm.obj)
```

1) Formulate the H0 and Ha hypotheses (using parameter notation) for testing whether the overall model is significant. Which part of summary() output corresponds to this test? Is the model significant?\
$H_0:\beta_1=...=\beta_7=0$\
$H_a:\{at~least~one~\beta_j\ne0,~j=1,...,7\}$\
The f-statistic section at the bottom corresponds to model significance. The model is statistically significant because the p-value ($\approx0$) is less than most common significance levels ($\alpha=0.001,0.01,0.05$).

2) Which predictors appear to have a statistically significant relationship to the response? Just list them.\
weight, year, and origin appear to have a statistically significant relationship to mpg at the $alpha=0.001$ level. In addition to the previous predictors, displacement appears to additionally have a statistically significant relationship to mpg at the $\alpha=0.01$ level. 

3) Interpret the effect of car’s weight on its miles per gallon.\
There is statistically significant linear relationship between weight (a predictor) and mpg (the response value) when holding all other predictors constant because the p-value ($\approx0$) is less than all common significance levels ($\alpha=0.001,0.01,0.05$).

4) For the effect from part 3, proceed to report and interpret the 95% confidence interval.
```{r}
confint(lm.obj)["weight",]
```
We are 95% confident that for 1 pound increase in weight, holding all other predictors constant, the average mpg will increase by between `r confint(lm.obj)["weight",][1]` and `r confint(lm.obj)["weight",][2]`.

# Is this really the right code?????????? ADD INTERPRETATION

5) Report and interpret both quality-of-fit metrics.
```{r eval=FALSE}
summary(lm.obj)$r.squared # R-Squared
summary(lm.obj)$sigma # RSE
```
  - $R^2$: Our linear regression model explains `r summary(lm.obj)$r.squared*100` percent of variety in mpg.
  - RSE: Our predicted mpg miss the true population mpg by `r summary(lm.obj)$sigma` miles, on average
  
# Problem #2

This problem will deal with cystfibr data example of ISwR package. In particular, we will be building a model to predict pemax (patient’s maximum respiratory pressure) based on other physical characteristics.

1) Proceed to fit the following multiple linear regression model: $pemax\sim~.$
```{r}
lm(pemax ~ ., data = ISwR::cystfibr) %>%
  lm.summary()
```

  a. Comment on the 1) overall model significance; 2) significance of any individual predictors. Why do you think this is happening (name the main issue)?\
The overall linear model is significant at the $\alpha=0.05$ level, indicating at least one important variable, despite no individual predictor being significant. This paradox is due to the issue of collinearity - some predictors are strongly correlated with each other.
  
  b. Proceed to address the issue observed in part (a) via studying a correlation matrix of predictors, modifying the model accordingly. Fit the modified model, comment on its 1) overall model significance; 2) significance of any individual predictors.
```{r eval=FALSE}
cor.summary(ISwR::cystfibr)
cor(ISwR::cystfibr)["pemax", c(1:3, 7, 8)] %>%
  kableExtra::kable(booktabs = True)
```
age, weight, and height are are strongly correlated with each other. Similarly, rv and frc are strongly correlated with each other. One one out of each strongly correlated predictor group needs to be retained; I chose the one with the strongest correlation to pemax, the response variable.
```{r eval=FALSE}
lm(pemax ~ . - age - height - rv, data = ISwR::cystfibr) %>%
  lm.summary()
```
The overall linear model is significant at the $\alpha=0.01$ level; bmp is significant at the $\alpha=0.05$ level; weight is significant at the $\alpha=0.001$ level. 

  c. Proceed to address the issue observed in part (a) via using VIF criteria method. Fit the modified model, comment on its 1) overall model significance; 2) significance of any individual predictors.
```{r}
car::vif(lm(pemax ~ . , data = ISwR::cystfibr))
car::vif(lm(pemax ~ . - weight, data = ISwR::cystfibr))
car::vif(lm(pemax ~ . - weight - frc, data = ISwR::cystfibr))
car::vif(lm(pemax ~ . - weight - frc - height, data = ISwR::cystfibr))
lm(pemax ~ . - weight - frc - height, data = ISwR::cystfibr) %>%
  lm.summary()
```
The overall linear model is significant at the $\alpha=0.05$ level; fev1 is significant at the $\alpha=0.05$ level; age is significant at the $\alpha=0.01$ level. 
  
  d. In you own words, why does collinearity prevent us from accurately estimating effects of collinear predictors on the response variable?\
  Collinearity prevents us from accurately estimating effects of collinear predictors on the response variable because it is tough to change one predictor while holding a collinear predictor constant since they are correlated. Even if there are observations which meet said criteria, the sample is small and therefore contains more uncertainty.
  
2) Proceed to fit the following two models:\
  - Full model: $pemax\sim~sex + weight + height + rv + frc$
  - Reduced model: $pemax\sim~sex + height + frc$ 
  
```{r eval=FALSE}
models <- list(lm(pemax ~ sex + weight + height + rv + frc, data = ISwR::cystfibr), lm(pemax ~ sex + height + frc, data = ISwR::cystfibr))
names(models) <- c("Full Model", "Reduced Model")
lm.summary(models)
```
  
  a. Comment on what happens to standard errors for $\hat{\beta_{height}}$ and $\hat{\beta_{frc}}$ coefficients when going from the full to reduced model. Why does this happen?
  
  b. Proceed to use VIF criteria in order to get from full model down to the reduced model. Which variable is dropped first? Second? Why?